{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TP 3 Regression logistique\nDans ce TP, nous aimerions prédire l'admission d'un étudiant à une specialité donnée selon ses notes dans deux matières.\n\nPour ce faire, nous étudierons un ensemble de données avec l'admission  (y) et les notes des deux modules (X).\n\nLa prédiction se fera avec l'agorithme de descente du gradient.","metadata":{}},{"cell_type":"markdown","source":"# TP réalisé par : \n* LABCHRI Amayas\n* KOULAL Yidhir Aghiles\n* BAROUD Yasmine\n* SENNIA Mohamed","metadata":{}},{"cell_type":"markdown","source":"# Importation des librairies necessaires au travail","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.079706Z","iopub.execute_input":"2022-03-24T23:47:52.080349Z","iopub.status.idle":"2022-03-24T23:47:52.105891Z","shell.execute_reply.started":"2022-03-24T23:47:52.080224Z","shell.execute_reply":"2022-03-24T23:47:52.105159Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Lecture des fichiers de données\nPour ce TP, nous allons lire les données à partir d'un fichier csv.","metadata":{}},{"cell_type":"code","source":"# données\ndata = np.genfromtxt('../input/datastp3/data.csv', delimiter=',')\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.107639Z","iopub.execute_input":"2022-03-24T23:47:52.108135Z","iopub.status.idle":"2022-03-24T23:47:52.126917Z","shell.execute_reply.started":"2022-03-24T23:47:52.108087Z","shell.execute_reply":"2022-03-24T23:47:52.126255Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Dans ces données (data), la première colonne represente la première note, la deuxieme colonne la deuxième note et la troisième colonne represente l'admission à la specialité (1 admis 0 non admis).\n\nChaque ligne represente un exemple de notre ensemble de données. \n\nMettons ces données dans leus vecteurs correspondants.","metadata":{}},{"cell_type":"code","source":"# rajoutons l'ordonnée à l'origine theta 0\nintercept=np.ones((data.shape[0],1))\nX=np.column_stack((intercept,data[:,0:2]))\ny = data[:, 2]","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.128007Z","iopub.execute_input":"2022-03-24T23:47:52.128624Z","iopub.status.idle":"2022-03-24T23:47:52.132946Z","shell.execute_reply.started":"2022-03-24T23:47:52.128594Z","shell.execute_reply":"2022-03-24T23:47:52.132392Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print('X', X.shape ,' y ', y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.134756Z","iopub.execute_input":"2022-03-24T23:47:52.135059Z","iopub.status.idle":"2022-03-24T23:47:52.146035Z","shell.execute_reply.started":"2022-03-24T23:47:52.135022Z","shell.execute_reply":"2022-03-24T23:47:52.145402Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"y  = y.reshape(y.shape[0], 1)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.147156Z","iopub.execute_input":"2022-03-24T23:47:52.147552Z","iopub.status.idle":"2022-03-24T23:47:52.156241Z","shell.execute_reply.started":"2022-03-24T23:47:52.147522Z","shell.execute_reply":"2022-03-24T23:47:52.155218Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.157613Z","iopub.execute_input":"2022-03-24T23:47:52.158230Z","iopub.status.idle":"2022-03-24T23:47:52.170148Z","shell.execute_reply.started":"2022-03-24T23:47:52.158182Z","shell.execute_reply":"2022-03-24T23:47:52.169527Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Descente du Gradient : Préparation des fonctions","metadata":{}},{"cell_type":"markdown","source":"00 - Hypothèse (model)","metadata":{}},{"cell_type":"markdown","source":"0- Fonction mpgistique (Sigmoid)","metadata":{}},{"cell_type":"code","source":"def h(X, theta):\n    return X.dot(theta)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.171192Z","iopub.execute_input":"2022-03-24T23:47:52.171965Z","iopub.status.idle":"2022-03-24T23:47:52.183214Z","shell.execute_reply.started":"2022-03-24T23:47:52.171924Z","shell.execute_reply":"2022-03-24T23:47:52.182506Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def Sigmoid(z):\n    # pour une valeur donnée, cette fonction calculera sa sigmoid\n    \n    return 1/ (1 + np.exp(-z.astype('float')))\n    \n ","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.184423Z","iopub.execute_input":"2022-03-24T23:47:52.184830Z","iopub.status.idle":"2022-03-24T23:47:52.196575Z","shell.execute_reply.started":"2022-03-24T23:47:52.184794Z","shell.execute_reply":"2022-03-24T23:47:52.195949Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"1- Calcul du coût\n\nCette fonction servira à calculer le cout $J(\\theta_0,\\theta_1)$\n\nElle prendra l'ensemble de données d'apprentissage en entrée ainsi que les paramètres définis initialement","metadata":{}},{"cell_type":"code","source":"def computeCost(X, y, theta):\n    # idéalement, tracer le coût à chaque itération pour s'assurer que la descente du gradient est correcte\n    \n    # calculer le coût avec et sans vectorisation, \n    # comparer le temps de traitement\n    m = len(y)\n    return (-1/m) * np.sum((y * np.log( Sigmoid( h(X,theta) ) )) + ((1 - y) * np.log(1 - Sigmoid( h(X, theta) ) )) ) \n  ","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.199075Z","iopub.execute_input":"2022-03-24T23:47:52.199926Z","iopub.status.idle":"2022-03-24T23:47:52.209250Z","shell.execute_reply.started":"2022-03-24T23:47:52.199885Z","shell.execute_reply":"2022-03-24T23:47:52.208258Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"2- Fonction de la descente du gradient\n\nCette fonction mettra à jour les paramètres $\\theta_0,\\theta_1$ jusqu'à convergence: atteinte du nombre d'itérations max, ou dérivée assez petite.","metadata":{}},{"cell_type":"markdown","source":"2.0 Gradient","metadata":{}},{"cell_type":"code","source":"def Gradient(X, y,theta):\n    m = len(y)\n    return (1 / m) * (X.T.dot(Sigmoid(h(X,theta)) - y ))","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.212972Z","iopub.execute_input":"2022-03-24T23:47:52.214052Z","iopub.status.idle":"2022-03-24T23:47:52.221774Z","shell.execute_reply.started":"2022-03-24T23:47:52.214004Z","shell.execute_reply":"2022-03-24T23:47:52.220912Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def gradientDescent(X, y, theta, alpha, iterations):\n    # garder aussi le cout à chaque itération\n    cost_array = np.zeros(iterations)\n    for i in range(iterations):\n        theta = theta - alpha * Gradient(X, y, theta)\n        cost_array[i] = computeCost(X, y, theta)\n    \n    return theta,  cost_array","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.222726Z","iopub.execute_input":"2022-03-24T23:47:52.223302Z","iopub.status.idle":"2022-03-24T23:47:52.235017Z","shell.execute_reply.started":"2022-03-24T23:47:52.223244Z","shell.execute_reply":"2022-03-24T23:47:52.233894Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Dessin de la limite de decision (Descision Boundary)\nDans cette partie, nous aimerions dessiner la ligne separatrice d nos données","metadata":{}},{"cell_type":"code","source":"def drawLine(X, y, theta,color_ligne):\n    x_values = [np.min(X[:, 1]), np.max(X[:, 1])]# besoin de seulement 2 points afin de dessiner une droite\n    y_values = - (theta[0] + theta[1]* x_values) / theta[2]\n    plt.figure(figsize = (10,6))\n    plt.scatter(X[np.where(y==1),1],X[np.where(y==1),2], label=\"accepte\",marker ='o')\n    plt.scatter(X[np.where(y==0),1],X[np.where(y==0),2], label=\"non accepte\",marker ='x')\n    plt.xlabel('Note module 1')\n    plt.ylabel('Note module 2')\n\n    plt.plot(x_values, y_values, color=color_ligne, label='Decision Boundary ')\n    plt.legend()\n    plt.title('Decision Boundary')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.236032Z","iopub.execute_input":"2022-03-24T23:47:52.236500Z","iopub.status.idle":"2022-03-24T23:47:52.247492Z","shell.execute_reply.started":"2022-03-24T23:47:52.236465Z","shell.execute_reply":"2022-03-24T23:47:52.246485Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Descente du Gradient : Appel des fonctions","metadata":{}},{"cell_type":"markdown","source":"Initialisation de $\\theta_0$ et $\\theta_1$","metadata":{}},{"cell_type":"code","source":"n=X.shape[1]\ntheta = np.random.rand(n, 1)\nprint(theta)\nsave_theta = theta","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.248459Z","iopub.execute_input":"2022-03-24T23:47:52.249076Z","iopub.status.idle":"2022-03-24T23:47:52.260212Z","shell.execute_reply.started":"2022-03-24T23:47:52.249042Z","shell.execute_reply":"2022-03-24T23:47:52.259539Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Déssiner la ligne","metadata":{}},{"cell_type":"code","source":"drawLine(X, y, theta, \"black\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.261339Z","iopub.execute_input":"2022-03-24T23:47:52.261847Z","iopub.status.idle":"2022-03-24T23:47:52.573008Z","shell.execute_reply.started":"2022-03-24T23:47:52.261814Z","shell.execute_reply":"2022-03-24T23:47:52.572150Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Calculer le cout initial","metadata":{}},{"cell_type":"code","source":"initialCost=computeCost(X, y, theta)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.574481Z","iopub.execute_input":"2022-03-24T23:47:52.575365Z","iopub.status.idle":"2022-03-24T23:47:52.579908Z","shell.execute_reply.started":"2022-03-24T23:47:52.575309Z","shell.execute_reply":"2022-03-24T23:47:52.578802Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"initialCost","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.581183Z","iopub.execute_input":"2022-03-24T23:47:52.581459Z","iopub.status.idle":"2022-03-24T23:47:52.594958Z","shell.execute_reply.started":"2022-03-24T23:47:52.581430Z","shell.execute_reply":"2022-03-24T23:47:52.594032Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Appel des la fonction de calcul du gradient","metadata":{}},{"cell_type":"code","source":"# paramètres\niterations = 1500\nalpha = 0.01\n# Appel\ntheta , couts= gradientDescent(X, y, save_theta, alpha, iterations)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.596193Z","iopub.execute_input":"2022-03-24T23:47:52.596541Z","iopub.status.idle":"2022-03-24T23:47:52.688763Z","shell.execute_reply.started":"2022-03-24T23:47:52.596502Z","shell.execute_reply":"2022-03-24T23:47:52.687896Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"theta.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.689929Z","iopub.execute_input":"2022-03-24T23:47:52.690161Z","iopub.status.idle":"2022-03-24T23:47:52.695315Z","shell.execute_reply.started":"2022-03-24T23:47:52.690133Z","shell.execute_reply":"2022-03-24T23:47:52.694727Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"theta","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.696322Z","iopub.execute_input":"2022-03-24T23:47:52.696672Z","iopub.status.idle":"2022-03-24T23:47:52.708730Z","shell.execute_reply.started":"2022-03-24T23:47:52.696643Z","shell.execute_reply":"2022-03-24T23:47:52.708188Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"FinalCost=computeCost(X, y, theta)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.709756Z","iopub.execute_input":"2022-03-24T23:47:52.710080Z","iopub.status.idle":"2022-03-24T23:47:52.719079Z","shell.execute_reply.started":"2022-03-24T23:47:52.710053Z","shell.execute_reply":"2022-03-24T23:47:52.718323Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"FinalCost","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.720191Z","iopub.execute_input":"2022-03-24T23:47:52.720698Z","iopub.status.idle":"2022-03-24T23:47:52.732743Z","shell.execute_reply.started":"2022-03-24T23:47:52.720662Z","shell.execute_reply":"2022-03-24T23:47:52.731955Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Le coût a diminué après application de la descente de gradient","metadata":{}},{"cell_type":"markdown","source":"Traçage de la fonction du coût","metadata":{}},{"cell_type":"code","source":"plt.plot(range(iterations), couts)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:52.733919Z","iopub.execute_input":"2022-03-24T23:47:52.734174Z","iopub.status.idle":"2022-03-24T23:47:53.057305Z","shell.execute_reply.started":"2022-03-24T23:47:52.734145Z","shell.execute_reply":"2022-03-24T23:47:53.056342Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Ce coût à l'air bon, mais l'est il vraiement.","metadata":{}},{"cell_type":"markdown","source":"Notons que $\\theta^T  x$ est équivalent à $X  \\theta $ où $X= \\begin{pmatrix}\n..(x^{(1)})^T..\\\\\n..(x^{(2)})^T..\\\\\n.\\\\\n.\\\\\n.\\\\\n..(x^{(m)})^T..\n\\end{pmatrix} $","metadata":{}},{"cell_type":"markdown","source":"# Affichage \nGraphe representant les acceptations selon les caracteristiques","metadata":{}},{"cell_type":"code","source":"pred = Sigmoid(h(X, theta))","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:53.058544Z","iopub.execute_input":"2022-03-24T23:47:53.058992Z","iopub.status.idle":"2022-03-24T23:47:53.063406Z","shell.execute_reply.started":"2022-03-24T23:47:53.058958Z","shell.execute_reply":"2022-03-24T23:47:53.062655Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"print(\"New model\")\ndrawLine(X, y, theta, \"red\")\nprint(\"Old model\")\ndrawLine(X, y, save_theta, \"black\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:53.067424Z","iopub.execute_input":"2022-03-24T23:47:53.067661Z","iopub.status.idle":"2022-03-24T23:47:53.579714Z","shell.execute_reply.started":"2022-03-24T23:47:53.067634Z","shell.execute_reply":"2022-03-24T23:47:53.578856Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Le model n'est pas performant, il suffit de voir la ligne séparatrice, **Il n'y a pas de classification!!!** entre nos données. Ce problème va être résolu dans la section **Renforcement d'apprentissage**.\nLa représentation du coût précedent ne voulait rien dire, il est probable que notre model ait subit un underfitting. Dans ce cas, la représentation du coût en 3D, Hypothèse seront mauvais.  ","metadata":{}},{"cell_type":"markdown","source":"Traçage du coût en fonction de theta0 et theta1","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objects as go\ndef Afficher_Sigmoid_3D(X, theta):\n    # Creer deux grille \n    X1, X2 = np.mgrid[0:20:0.125, 0:20:0.125]\n    X0 = np.ones(X1.shape)\n    # Calcule d'un vecteur Hyphotèses\n    Z=np.divide(1,1+np.exp(-1*(np.multiply(theta[0],X0)+np.multiply(theta[1],X1)+np.multiply(theta[2],X2)) ))\n    #Afficher la figure\n    fig = go.Figure(data = go.Surface(\n    z = Z,\n    x = X1,\n    y = X2\n    )\n    )\n    # Ajouter les valeurs à la figure\n    fig.update_traces(contours_z=dict(show=True, usecolormap = True, project_z=True, color=\"red\"))\n    fig.update_layout(title = \"Model_For_Logistic_regression\", scene = dict(\n    xaxis_title=\"Theta 0\",\n    yaxis_title=\"Theta 1\",\n    zaxis_title=\"Model\"\n    ),\n    width = 500, \n    height = 500\n    )\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:53.580978Z","iopub.execute_input":"2022-03-24T23:47:53.581204Z","iopub.status.idle":"2022-03-24T23:47:53.599702Z","shell.execute_reply.started":"2022-03-24T23:47:53.581176Z","shell.execute_reply":"2022-03-24T23:47:53.598984Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"Afficher_Sigmoid_3D( X, theta)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:53.600891Z","iopub.execute_input":"2022-03-24T23:47:53.601186Z","iopub.status.idle":"2022-03-24T23:47:53.836820Z","shell.execute_reply.started":"2022-03-24T23:47:53.601142Z","shell.execute_reply":"2022-03-24T23:47:53.836210Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Comme nous l'avons dit précédement, **ce model ne vaut rien**, il ne nous servira pas pour notre classification.","metadata":{}},{"cell_type":"markdown","source":"coût en 3D","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objects as go\ndef Plot3DCosts(X, y, theta, alpha,iteration):\n    # Calculer les teta pour chaque iteration\n    X_theta = np.zeros(iteration)\n    Y_theta = np.zeros(iteration)\n    theta = np.zeros_like(theta)\n    couts = np.zeros(iteration)\n    for i in range(0, iteration):\n        X_theta[i] = (theta[0])\n        Y_theta[i] = (theta[1])\n        ## Calculer la descente de gradient pour une iteration\n        theta , couts= gradientDescent(X, y, theta, alpha, 1)\n\n    ## Créer nos matrice pour theta0 et theta1\n    X_theta = X_theta[1:]\n    Y_theta = Y_theta[1:]\n    X_theta = np.array([X_theta[i]  for i in range(0, iteration, 5)])\n    Y_theta = np.array([Y_theta[i]  for i in range(0, iteration, 5)])\n    #X1, X2 = np.mgrid[0:20:0.125, 0:20:0.125]\n    X_2D = np.zeros((X_theta.shape[0],X_theta.shape[0]))\n    Y_2D = np.zeros((Y_theta.shape[0],Y_theta.shape[0]))\n    Z_2D = np.zeros((Y_theta.shape[0],Y_theta.shape[0]))\n    \n    # Calculer les couts \n    for i in range(X_2D.shape[0]):\n        for j in range(X_2D.shape[1]):\n            X_2D[i][j] = X_theta[i]\n            Y_2D[j][i] = Y_theta[i]\n    \n    for i in range(X_2D.shape[0]):\n        for j in range(X_2D.shape[1]):\n            Z_2D[i][j] = computeCost(X,y , [X_2D[i][j], Y_2D[i][j], theta[2]])\n    #print(Z_2D.shape)\n    fig = go.Figure(data = go.Surface(\n    z = Z_2D,\n    x = X_2D,\n    y = Y_2D\n    )\n    )\n    \n    fig.update_traces(contours_z=dict(show=True, usecolormap = True, project_z=True))\n    fig.update_layout(title = \"Cost_Function_Plot\", scene = dict(\n    xaxis_title=\"Theta 0\",\n    yaxis_title=\"Theta 1\",\n    zaxis_title=\"Cost Function Values\"\n    ),\n    width = 500, \n    height = 500\n    )\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:53.838020Z","iopub.execute_input":"2022-03-24T23:47:53.838391Z","iopub.status.idle":"2022-03-24T23:47:53.854463Z","shell.execute_reply.started":"2022-03-24T23:47:53.838349Z","shell.execute_reply":"2022-03-24T23:47:53.853573Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"Plot3DCosts(X, y, theta, 0.01,iterations)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:47:53.856058Z","iopub.execute_input":"2022-03-24T23:47:53.856478Z","iopub.status.idle":"2022-03-24T23:49:15.359915Z","shell.execute_reply.started":"2022-03-24T23:47:53.856435Z","shell.execute_reply":"2022-03-24T23:49:15.358664Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Dessin 3D","metadata":{}},{"cell_type":"code","source":"\"\"\"\nExplication : \nLa fonction Afficher_Logistic_pour_Grad : calculer le gradient  (n itération) fois pour une seul itération pour obtenir un theta d'une itération.\nafin d'afficher les variation du model.\n\nAfficher_model_pour_dataset : Affiche notre model pour chaque descente de gradient appliqué\n\n\"\"\"\ndef Afficher_model_pour_dataset(X,Y,t0,t1,t2):\n    fig = plt.figure(1)\n    X1, X2 = X[:,1], X[:,2] # les notes des modules\n    ax = fig.add_subplot(111,projection='3d') # figure en 3d\n    # Afficher les données du dataset\n    surf = ax.scatter(X[np.where(y==1),1], X[np.where(y==1),2], np.ones(X[np.where(y==1),1].shape) ,c='b',marker='o')\n    surf = ax.scatter(X[np.where(y==0),1], X[np.where(y==0),2], np.zeros(X[np.where(y==0),1].shape) ,c='r',marker='x')\n   # plt.show() \n    # Varier les points de 0  à 20\n    GX1, GX2 = np.mgrid[0:20:0.5, 0:20:0.5]\n    # Bias\n    GX0 = np.ones(GX1.shape)\n    # Hypothèse\n    Z = np.divide(1,1+np.exp(-1*(np.multiply(t0,GX0)+np.multiply(t1,GX1)+np.multiply(t2,GX2))))\n    # dessiner le model \n    surf = ax.plot_wireframe(GX1, GX2, Z, cstride=1, rstride=1)\n    \n    ##---Rotation de la figure----\n    ax.view_init(30,120)\n    plt.xlabel(\"Note module 1\")\n    plt.ylabel(\"Note module 2\")\n    ax.set_zlabel(\"Prediction\")\n    plt.show()\ndef Afficher_Logistic_pour_Grad(X,y,theta, alpha,times):\n    for i in range(times + 1):\n        theta , c = gradientDescent(X,y,theta,alpha,1)\n        if (i % 100 == 0):# afficher chaque 100 iterations\n            print(\"Itération = \",i,\"\\n\")\n            Afficher_model_pour_dataset(X,y,theta[0],theta[1],theta[2])","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:49:15.361692Z","iopub.execute_input":"2022-03-24T23:49:15.362215Z","iopub.status.idle":"2022-03-24T23:49:15.380056Z","shell.execute_reply.started":"2022-03-24T23:49:15.362164Z","shell.execute_reply":"2022-03-24T23:49:15.379152Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"Afficher_Logistic_pour_Grad(X,y,theta,alpha,1500)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:49:15.381075Z","iopub.execute_input":"2022-03-24T23:49:15.381578Z","iopub.status.idle":"2022-03-24T23:49:19.441895Z","shell.execute_reply.started":"2022-03-24T23:49:15.381543Z","shell.execute_reply":"2022-03-24T23:49:19.441020Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Prédire des valeurs de y","metadata":{}},{"cell_type":"code","source":"# Predire pour des notes note1= 9 et note2=17\nnote1 = 9\nnote2 = 17\nc = np.array([1,9,17])\nprint(\"La porbabilitée qu'un étuidiant puisse être admis en ayant un\",note1,\"dans le module 1 et un\",note2,\"dans le module 2 est :\",np.round(Sigmoid(h(c, theta)), 2)[0] * 100,\"%\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:49:19.443303Z","iopub.execute_input":"2022-03-24T23:49:19.443523Z","iopub.status.idle":"2022-03-24T23:49:19.451386Z","shell.execute_reply.started":"2022-03-24T23:49:19.443497Z","shell.execute_reply":"2022-03-24T23:49:19.450463Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Vérification de l'implementation\nComparer vos algorithmes à ceux de scikitlearn","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression( C = 1e21).fit(X[:,1:3], y)\nprint(clf.intercept_, clf.coef_)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:49:19.452758Z","iopub.execute_input":"2022-03-24T23:49:19.453083Z","iopub.status.idle":"2022-03-24T23:49:20.702082Z","shell.execute_reply.started":"2022-03-24T23:49:19.453039Z","shell.execute_reply":"2022-03-24T23:49:20.701483Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"theta_sklearn = np.array([[clf.intercept_[0]],[ clf.coef_[0][0]], [clf.coef_[0][1]]])","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:49:20.702963Z","iopub.execute_input":"2022-03-24T23:49:20.703301Z","iopub.status.idle":"2022-03-24T23:49:20.708382Z","shell.execute_reply.started":"2022-03-24T23:49:20.703253Z","shell.execute_reply":"2022-03-24T23:49:20.707426Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"theta_sklearn","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:49:20.709732Z","iopub.execute_input":"2022-03-24T23:49:20.710024Z","iopub.status.idle":"2022-03-24T23:49:20.722327Z","shell.execute_reply.started":"2022-03-24T23:49:20.709983Z","shell.execute_reply":"2022-03-24T23:49:20.721469Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Renforcement d'apprentissage\nMettre ici toute idée qui pourrait renforcer votre apprentissage\n\nNous avons calculer notre theta pour un alpha = 0.01 et 1500 itérations. Le résultat n'a pas était satisfaisant, c'est pour celà que nous allons appliquer différents tests afin d'atteindre un objectif bien précis.\n\nVu que nous avons comparé notre theta avec celui de Sklearn, nous allons fixer comme objectif d'obtenir un theta plus au moins proche de celui de Sklearn qui nous donnera de meilleurs résultats.\n\n**Définir un plan de travail** : \nDans ce plan de travail, nous noterons pour chaque execution les résultats obtenu. Nous travaillerons en 2 phase :\n* **Phase 1**: Durant cette phase nous changerons le nombre d'itérations en gardant un alpha fixe (0.01), nous discuterons les résultats.\n\n* **Phase 2**: Nous repreduirons le travail effectué dans la phase 1 mais cette fois en variant notre alpha et on fixant le nombre d'itérations (meilleur valeur dans phase 1).\n\n\n\n**Résultats Phase 1**: Calculer theta pour un nombre d'itération varier :\n\n* **5000 itérations**  :**coût = 0.47** le plus grand coût parmis les 3 tests. \n* **7000 itérations**  :**coût = 0.36** moins que celui obtenu après la première application de la descente de gradient, l'erruer ne semble pas diminuer selon son graphe 2D et pour finir le model a une allure sigmoid mais qui ne lui ressemble pas vraiment, peut être que modifer **alpha** aura un bon effet sur ce model.\n* **50000 itérations** : **coût =  0.24** le coût le plus bas atteint, le model à une forme sigmoid et la courbe de l'erreur tant plus vers 0. Pour la ligne séparatrice elle n'est pas encore fiable.\n\n**Choix du model** : Selon les résultats obtenu, nous avons choisis de garder le model avec **50000 itérations** car on a obtenu un faible coût avec ce model. Le fait d'avoir une ligne séparatrice meilleur que celle du model avec **7000 itérations** nous a permis d'éliminer ce dernier. \n\n**Résultats Phase 1**: Calculer theta pour différentes valeurs d'alpha : \n**On précise que pour l'instant, notre meilleur model à une configuration (alpha, iteration) = (0.01, 50000) avec un coût de 0.24**.\n\n* **0.9**    :**coût = nan**, ce paramétre ne sera pas pris, pas besoin de sétalé dessus.\n* **0.55**   :**coût = nan**, le model est refusé.\n* **0.204**  :**coût = 0.3378**, c'est un bon model, sauf qu'il a un coût un peu plus élevé que notre meilleur model jusqu'à maintenant.\n* **0.202**  :**coût = 0.264**, un très bon model, il est deuxième au classement pour le moment. \n* **0.104**  :**coût = 0.2034** Excellent, on a pu diminuer encore une fois le coût de notre model. \n* **0.10**   :**coût = 0.2035** Bon model, mais avec un coût un peu plus grand que celui vu parecedement.\n\n**Pour cette phase, nous avons obtenu des models assez interéssants car non seulement leurs coût était très faible, mais même la ligne séparatrice de chaqu'un d'eux était assez d'escriminente pour notre dataset**. \n\n***Conclusion*** : \nNous avons obtenu un model avec un coût de 0.24 pour un **alpha = 0.104** et un **nombre d'itérations = 50000**.\nNous allons de suite calculer la valeur de theta et le comparer avec celui de SKlearn.","metadata":{}},{"cell_type":"markdown","source":"## Définir des fonctions pour les résultats : ","metadata":{}},{"cell_type":"code","source":"def CalculeCoutVariationIteration(X,y,theta,alpha, tab_iterations):\n    # Création d'une copy de theta\n    theta_iteration = np.zeros_like(theta)\n    # Calculer la descente de gradient pour chaque itérations dans le tableau d'itérations\n    for iteration in tab_iterations:\n        print(\"Calculer theta pour :\", iteration,\" itérations\")\n        # Initilaliser un coût local\n        couts_local = np.zeros((iteration))\n        \n        #Calculer la descente de gradient pour une iteration\n        theta_iteration, couts_local = gradientDescent(X, y, theta_iteration, alpha, iteration)\n        cout_iteration  = computeCost(X, y, theta_iteration)\n        print(\"Le coût pour \", iteration,\" iterations est : \", cout_iteration)\n        # Afficher notre model en 3D\n        print(\"Affichage du model en 3D , couts 2D pour \", iteration,\" itérations\")\n        Afficher_Sigmoid_3D( X, theta_iteration)\n        # Afficher coût en 2D\n        #print(\"Affichage du coût en 2D pour \", iteration,\" itérations\")\n        plt.figure(num = iteration)\n        plt.plot(range(iteration), couts_local)\n        plt.title(\"Coût avec \"+str(iteration)+\" itérations\")\n        # Afficher la ligne séparatrice (Boundary line) pour notre theta_iteration et notre ancien theta\n        \n        drawLine(X, y, theta_iteration, \"red\")\n        #print(\"Ligne séparatrice pour 1500 itérations\")\n        drawLine(X, y, theta, \"black\")\n        \ndef CalculeCoutVariationAlpha(X,y,theta,tab_alpha, iterations):\n    # Création d'une copy de theta\n    theta_alpha = np.zeros_like(theta)\n    # Calculer la descente de gradient pour chaque itérations dans le tableau d'alpha\n    i = 0\n    for alpha in tab_alpha:\n        print(\"Calculer theta pour alpha = \", alpha)\n        # Initilaliser un coût local\n        couts_local = np.zeros((iterations))\n        \n        #Calculer la descente de gradient pour un alpha\n        theta_alpha, couts_local = gradientDescent(X, y, theta, alpha, iterations)\n        cout_alpha  = computeCost(X, y, theta_alpha)\n        print(\"Le coût pour un alpha = \",alpha,\" est : \", cout_alpha)\n        # Afficher notre model en 3D\n        print(\"Affichage du model en 3D , couts 2D pour  alpha = \", alpha)\n        Afficher_Sigmoid_3D( X, theta_alpha)\n        # Afficher coût en 2D\n        #print(\"Affichage du coût en 2D pour \", iteration,\" itérations\")\n        plt.figure(num = i)\n        plt.plot(range(iterations), couts_local)\n        plt.title(\"Coût avec alpha = \"+str(alpha))\n        # Afficher la ligne séparatrice (Boundary line) pour notre theta_iteration et notre ancien theta\n        \n        drawLine(X, y, theta_alpha, \"red\")\n        #print(\"Ligne séparatrice pour 1500 itérations\")\n        drawLine(X, y, theta, \"black\")\n        i = i +1\n        \n\n    \n        \n\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:49:20.723709Z","iopub.execute_input":"2022-03-24T23:49:20.724180Z","iopub.status.idle":"2022-03-24T23:49:20.739629Z","shell.execute_reply.started":"2022-03-24T23:49:20.724139Z","shell.execute_reply":"2022-03-24T23:49:20.738706Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"iterations_vector = [5000,7000,50000]\n\nCalculeCoutVariationIteration(X,y,theta,alpha, iterations_vector)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:49:20.740819Z","iopub.execute_input":"2022-03-24T23:49:20.741145Z","iopub.status.idle":"2022-03-24T23:49:26.294048Z","shell.execute_reply.started":"2022-03-24T23:49:20.741118Z","shell.execute_reply":"2022-03-24T23:49:26.293105Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"vector_alpha = [0.9,0.55,0.204,0.202,0.104,0.10]\nCalculeCoutVariationAlpha(X,y,theta,vector_alpha, 50000)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:49:26.295537Z","iopub.execute_input":"2022-03-24T23:49:26.295855Z","iopub.status.idle":"2022-03-24T23:49:47.656320Z","shell.execute_reply.started":"2022-03-24T23:49:26.295816Z","shell.execute_reply":"2022-03-24T23:49:47.655323Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"#Calculer le temps pris pour calculer le gradient pour 50000 itérations\nimport datetime\na = datetime.datetime.now().replace(microsecond=0)\nfinal_theta, final_cost = gradientDescent(X,y,theta,0.104, 50000)\nb = datetime.datetime.now().replace(microsecond=0)\ntemps_gradient_descente = b - a\nprint(\"le temps pris du calcul de la descenete de gradient pour 50000 itérations est \", temps_gradient_descente,\" microsecondes\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:49:47.657528Z","iopub.execute_input":"2022-03-24T23:49:47.657754Z","iopub.status.idle":"2022-03-24T23:49:50.417137Z","shell.execute_reply.started":"2022-03-24T23:49:47.657728Z","shell.execute_reply":"2022-03-24T23:49:50.416417Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"print(\"Notre theta final = \\n\",final_theta,\"\\ntheta de Sklearn = \\n\",theta_sklearn)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:49:50.418436Z","iopub.execute_input":"2022-03-24T23:49:50.418675Z","iopub.status.idle":"2022-03-24T23:49:50.424380Z","shell.execute_reply.started":"2022-03-24T23:49:50.418645Z","shell.execute_reply":"2022-03-24T23:49:50.423559Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"**Les tableaux ne sont pas identiques à 100% mais ils ont des valeurs très proches** ","metadata":{}},{"cell_type":"markdown","source":"## Dessiner le coût","metadata":{}},{"cell_type":"code","source":"plt.figure(1)\nplt.plot(range(50000), final_cost)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:49:50.425570Z","iopub.execute_input":"2022-03-24T23:49:50.425810Z","iopub.status.idle":"2022-03-24T23:49:50.657435Z","shell.execute_reply.started":"2022-03-24T23:49:50.425776Z","shell.execute_reply":"2022-03-24T23:49:50.656529Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## Dessiner le model 3D","metadata":{}},{"cell_type":"code","source":"Afficher_Sigmoid_3D(X, final_theta)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:49:50.658519Z","iopub.execute_input":"2022-03-24T23:49:50.658730Z","iopub.status.idle":"2022-03-24T23:49:50.697872Z","shell.execute_reply.started":"2022-03-24T23:49:50.658705Z","shell.execute_reply":"2022-03-24T23:49:50.696998Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"## Dessiner les variations du model en 3D","metadata":{}},{"cell_type":"code","source":"Afficher_Logistic_pour_Grad(X,y,theta,0.104,50000)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:49:50.699066Z","iopub.execute_input":"2022-03-24T23:49:50.699317Z","iopub.status.idle":"2022-03-24T23:51:55.374969Z","shell.execute_reply.started":"2022-03-24T23:49:50.699263Z","shell.execute_reply":"2022-03-24T23:51:55.374253Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def normalize(X):\n    norm = np.copy(X)\n    moyenne = np.mean(norm[:,1:2])\n    std= np.std(norm[:,1:2])\n    norm[:,1] = (norm[:,1] - moyenne)/(std)\n    norm[:,2] = (norm[:,2] - moyenne)/(std)\n\n  ## Ajouter un bias\n  #norm = np.column_stack((np.ones(norm.shape[0]), norm ))\n\n    return norm\n                                                    \n        \n\ndef Scaling(X):\n    scale = np.copy(X)\n    scale[:,2] = (X[:,2] - np.min(X[:,2]))/(np.max(X[:,2])  -  np.min(X[:,2]))\n    scale[:, 1] = (X[:,1] - np.min(X[:,1]))/(np.max(X[:,1])  -  np.min(X[:,1]))\n\n    return scale\n\ndef ScalingY(X):\n    scale = np.copy(X)\n    scale = (X[:] - np.min(X))/(np.max(X)  -  np.min(X))\n  \n    return scale","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:51:55.375969Z","iopub.execute_input":"2022-03-24T23:51:55.376170Z","iopub.status.idle":"2022-03-24T23:51:55.386617Z","shell.execute_reply.started":"2022-03-24T23:51:55.376143Z","shell.execute_reply":"2022-03-24T23:51:55.385527Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Normalisation de X et mise à l'échelle de y","metadata":{}},{"cell_type":"code","source":"import datetime\nit = 5000\nX_scale = normalize(X)\ny_scale = ScalingY(y)\na_s = datetime.datetime.now().replace(microsecond=0)\ntheta_vectorizaton, c=  gradientDescent(X_scale,y_scale, theta, 0.104,it)\nb_s = datetime.datetime.now().replace(microsecond=0)\ntemps_vectorization = b_s - a_s\nprint(\"le temps pris du calcul de la descenete de gradient  pour \",it,\" itérations est \", temps_vectorization,\" microsecondes\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:51:55.388197Z","iopub.execute_input":"2022-03-24T23:51:55.388466Z","iopub.status.idle":"2022-03-24T23:51:55.666335Z","shell.execute_reply.started":"2022-03-24T23:51:55.388438Z","shell.execute_reply":"2022-03-24T23:51:55.665431Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"computeCost(X_scale,y_scale, theta_vectorizaton)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:51:55.667766Z","iopub.execute_input":"2022-03-24T23:51:55.668075Z","iopub.status.idle":"2022-03-24T23:51:55.674929Z","shell.execute_reply.started":"2022-03-24T23:51:55.668034Z","shell.execute_reply":"2022-03-24T23:51:55.674340Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"drawLine(X_scale,y_scale,theta_vectorizaton, \"green\")\ndrawLine(X,y,final_theta, \"red\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:51:55.676575Z","iopub.execute_input":"2022-03-24T23:51:55.677146Z","iopub.status.idle":"2022-03-24T23:51:56.198332Z","shell.execute_reply.started":"2022-03-24T23:51:55.677103Z","shell.execute_reply":"2022-03-24T23:51:56.197362Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"On a pu obtenir la même courbe séparatrice  et dans un temps 2x moins en faisant un vectrization de nos données","metadata":{}},{"cell_type":"markdown","source":"# Stochastic Gradient Descent :\n\nOn calcule le gradient pour n itérations seulement cette fois pour un exemple aléatoire de notre dataset, c'est une des variation de la descente de gradient.\nUn processus **stochastique** signifie un processus random en théorie des probabilités","metadata":{}},{"cell_type":"code","source":"def StothasticGradientDescent(X, y, theta,alpha, iterations):\n    \n    \n    m = X.shape[0]\n    #X  = normalize(X)\n    #y = ScalingY(y)\n    for i in range(iterations):\n        vectuer_index = np.random.randint(0,m, m+1)\n        for index in vectuer_index:\n            Xn = X[index:index + 1]\n            yn = y[index:index + 1]\n            theta = theta - alpha * Gradient(Xn,yn,theta)\n            \n        \n    return theta","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:51:56.199580Z","iopub.execute_input":"2022-03-24T23:51:56.199955Z","iopub.status.idle":"2022-03-24T23:51:56.206875Z","shell.execute_reply.started":"2022-03-24T23:51:56.199921Z","shell.execute_reply":"2022-03-24T23:51:56.205960Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"import datetime\niterations_st = 9700\nalpha_st = 0.02\nX_scale = normalize(X)\ny_scale = ScalingY(y)\na_s = datetime.datetime.now().replace(microsecond=0)\ntheta_stochastic=  StothasticGradientDescent(X_scale,y, theta,alpha_st,iterations_st)\nb_s = datetime.datetime.now().replace(microsecond=0)\ntemps_sothastic = b_s - a_s\nprint(\"le temps pris du calcul de la descenete de gradient sthotastic pour \",it,\" itérations est \", temps_sothastic,\" microsecondes\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:53:45.724612Z","iopub.execute_input":"2022-03-24T23:53:45.724923Z","iopub.status.idle":"2022-03-24T23:54:01.489191Z","shell.execute_reply.started":"2022-03-24T23:53:45.724889Z","shell.execute_reply":"2022-03-24T23:54:01.488259Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"theta_stochastic","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:54:15.983254Z","iopub.execute_input":"2022-03-24T23:54:15.983537Z","iopub.status.idle":"2022-03-24T23:54:15.989160Z","shell.execute_reply.started":"2022-03-24T23:54:15.983509Z","shell.execute_reply":"2022-03-24T23:54:15.988599Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"computeCost(X_scale, y, theta_stochastic)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:54:17.333364Z","iopub.execute_input":"2022-03-24T23:54:17.333971Z","iopub.status.idle":"2022-03-24T23:54:17.340428Z","shell.execute_reply.started":"2022-03-24T23:54:17.333922Z","shell.execute_reply":"2022-03-24T23:54:17.339613Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"computeCost(X, y, final_theta)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:54:18.503264Z","iopub.execute_input":"2022-03-24T23:54:18.504100Z","iopub.status.idle":"2022-03-24T23:54:18.510647Z","shell.execute_reply.started":"2022-03-24T23:54:18.504052Z","shell.execute_reply":"2022-03-24T23:54:18.509950Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"## Différence entre le coût du model précedent et celui avec la stochastic gradient descente","metadata":{}},{"cell_type":"code","source":"computeCost(X_scale, y, theta_stochastic) - computeCost(X, y, final_theta)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:54:20.140773Z","iopub.execute_input":"2022-03-24T23:54:20.141138Z","iopub.status.idle":"2022-03-24T23:54:20.147752Z","shell.execute_reply.started":"2022-03-24T23:54:20.141100Z","shell.execute_reply":"2022-03-24T23:54:20.147134Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"drawLine(X_scale,y_scale,theta_stochastic, \"green\")\ndrawLine(X,y,final_theta, \"red\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T23:54:24.338531Z","iopub.execute_input":"2022-03-24T23:54:24.338923Z","iopub.status.idle":"2022-03-24T23:54:24.852991Z","shell.execute_reply.started":"2022-03-24T23:54:24.338894Z","shell.execute_reply":"2022-03-24T23:54:24.851742Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"Avec la descenete de gradient stotastic on a pu avoir une meme courbe de decision en moins d'itérations et pour quelques exemples de notre dataset.","metadata":{}}]}